{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lunar-lander.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPx5I7t8xLUB9LbDxnf+wDT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"SQYR5y8A4HhU"},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install colabgymrender==1.0.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p8PEsu4a4iVY"},"source":["! wget http://www.atarimania.com/roms/Roms.rar\n","! mkdir /content/ROM/\n","! unrar e /content/Roms.rar /content/ROM/\n","! python -m atari_py.import_roms /content/ROM/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rj30DGbT4lZP"},"source":["!pip3 install Box2D\n","!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","\n","import gym\n","from gym import envs\n","from colabgymrender.recorder import Recorder\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","from tensorflow.keras.optimizers import Adam"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_93NcOy4qjU","executionInfo":{"status":"ok","timestamp":1632388372775,"user_tz":-330,"elapsed":419,"user":{"displayName":"Rishabh D A","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08186196616710850135"}},"outputId":"4a7807c1-0016-4acb-98d0-0ccdb04ed61f"},"source":["env = gym.make(\"LunarLander-v2\")\n","#######saved model\n","# 8,256,256,4\n","#adam mse lr=0.001\n","\n","def create_model():\n","  inputs = layers.Input(shape=(8))# - actual used for saved mode\n","  layer1 = layers.Dense(256,activation='relu')(inputs)#\n","  layer2 = layers.Dense(256,activation='relu')(layer1)\n","  action = layers.Dense(4)(layer2)\n","\n","  return keras.Model(inputs=inputs, outputs=action)\n","\n","model = create_model()\n","model.compile(optimizer=keras.optimizers.Adam(lr=0.001),loss=\"mean_squared_error\")\n","target_model = create_model()\n","target_model.compile(optimizer=keras.optimizers.Adam(lr=0.001),loss=\"mean_squared_error\")\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 8)]               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               2304      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 256)               65792     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 4)                 1028      \n","=================================================================\n","Total params: 69,124\n","Trainable params: 69,124\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"jgqaBCT3BCd3"},"source":["\n","total_rewards = 0\n","frame_count = 0\n","episode_count = 0\n","max_memory_length = 100000\n","update_after = 4\n","update_target_after = 200\n","epsilon = 1\n","gamma = 0.99\n","\n","action_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","done_history = []\n","episode_reward_history = []\n","flag=0\n","for i in range(1000):\n","  state = env.reset()\n","  state = np.array(state)\n","  rewards=0\n","\n","  for step in range(1,250):\n","    frame_count+=1\n","\n","    if frame_count < 15000 or epsilon > np.random.rand(1)[0]:\n","      action = np.random.choice(4)\n","    else:\n","      state_tensor = tf.convert_to_tensor(state)\n","      state_tensor = tf.expand_dims(state_tensor,0)\n","      action_prob = model.predict(state_tensor)\n","      action = tf.argmax(action_prob[0]).numpy()\n","\n","    epsilon -= 0.99/100000\n","    epsilon = max(epsilon,0.1)\n","\n","    state_next,reward,done,info = env.step(action)\n","    state_next = np.array(state_next)\n","\n","    rewards += reward\n","\n","    action_history.append(action)\n","    state_history.append(state)\n","    state_next_history.append(state_next)\n","    done_history.append(done)\n","    rewards_history.append(reward)\n","\n","    state = state_next\n","\n","    if len(done_history) > 32:\n","      indices = np.random.choice(range(len(done_history)),size=32)\n","\n","      state_sample = np.array([state_history[i] for i in indices])\n","      state_next_sample = np.array([state_next_history[i] for i in indices])\n","      rewards_sample = [rewards_history[i] for i in indices]\n","      action_sample = [action_history[i] for i in indices]\n","      done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n","\n","      future_rewards = target_model.predict(state_next_sample)\n","      current_rewards = model.predict(state_sample)\n","      updated_q = current_rewards[:]\n","\n","      for idx,terminal in enumerate(done_sample):\n","        if terminal:\n","          future_rewards[idx]=0.0\n","        updated_q[idx,action_sample[idx]] = rewards_sample[idx] + gamma*tf.reduce_max(future_rewards,axis=1)[idx]\n","\n","      model.train_on_batch(state_sample,updated_q)\n","\n","    if frame_count % update_target_after ==0:\n","      target_model.set_weights(model.get_weights())\n","\n","    if len(rewards_history)>max_memory_length:\n","      del rewards_history[:1]\n","      del state_history[:1]\n","      del state_next_history[:1]\n","      del action_history[:1]\n","      del done_history[:1]\n","\n","    if done:\n","      break\n","\n","  print(\"episode reward:\",rewards,\"mean reward\",total_rewards,\"episode:\",episode_count,\"frame:\",frame_count)\n","  episode_reward_history.append(rewards)\n","  if len(episode_reward_history)>100:\n","    del episode_reward_history[:1]\n","  episode_count+=1\n","  total_rewards = np.mean(episode_reward_history)\n","\n","  if total_rewards > 180:\n","    print(\"solved at episode: \",episode_count)\n","    break\n","#env.play()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-B0kFFRc781","executionInfo":{"status":"ok","timestamp":1632403875230,"user_tz":-330,"elapsed":370795,"user":{"displayName":"Rishabh D A","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08186196616710850135"}},"outputId":"726660cf-36e9-4c16-e62d-b0c160d3ed6c"},"source":["directory = './video'\n","env = Recorder(env, directory)\n","for c in range(10):\n","  obs=env.reset()\n","  done = False\n","  reward=0\n","  while not done:\n","    obs = tf.convert_to_tensor([obs])\n","    a=model.predict(obs)\n","    a = tf.argmax(a[0]).numpy()\n","    obs,r,done,_ = env.step(a)\n","    reward+=r\n","  print(reward)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["68.4016121850942\n","184.16714512821002\n","196.78437876046868\n","222.33239653628007\n","220.7553658017827\n","184.25404453789554\n","169.42569775289877\n","239.96258486219287\n","152.8429748584595\n","72.99544251871755\n"]}]},{"cell_type":"code","metadata":{"id":"_hteL7QLmemH"},"source":["directory = './video'\n","env = Recorder(env, directory)\n","\n","observation = env.reset()\n","terminal = False\n","while not terminal:\n","  state_tensor = tf.convert_to_tensor(observation)\n","  state_tensor = tf.expand_dims(state_tensor,0)\n","  action_prob = model.predict(state_tensor)\n","  action = tf.argmax(action_prob[0]).numpy()\n","  observation, reward, terminal, info = env.step(action)"],"execution_count":null,"outputs":[]}]}